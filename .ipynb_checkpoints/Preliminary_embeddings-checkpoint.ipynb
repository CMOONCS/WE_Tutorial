{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1]\n",
      " [1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "CORPUS = [\n",
    "'the green car is fast',\n",
    "'The blue car has low mileage',\n",
    "]\n",
    "### BOG model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def bow_generator(corpus, ngram_range=(1,2)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    " \n",
    " \n",
    "# build bow vectorizer and get features\n",
    "bow_vectorizer, bow_features = bow_generator(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0]\n",
      " [0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0\n",
      "  0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  1 1]\n",
      " [1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
      "  0 0]]\n"
     ]
    }
   ],
   "source": [
    "CORPUS = [\n",
    "'the green car is fast',\n",
    "'The blue car has low mileage',\n",
    "'Your car has high mileage',\n",
    "'I buy green shirt because it fits my blue eyes'\n",
    "]\n",
    "\n",
    "def bow_generator(corpus, ngram_range=(1,2)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    " \n",
    "# build bow vectorizer and get features\n",
    "bow_vectorizer, bow_features = bow_generator(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['because', 'because it', 'blue', 'blue car', 'blue eyes', 'buy', 'buy green', 'car', 'car has', 'car is', 'eyes', 'fast', 'fits', 'fits my', 'green', 'green car', 'green shirt', 'has', 'has high', 'has low', 'high', 'high mileage', 'is', 'is fast', 'it', 'it fits', 'low', 'low mileage', 'mileage', 'my', 'my blue', 'shirt', 'shirt because', 'the', 'the blue', 'the green', 'your', 'your car']\n"
     ]
    }
   ],
   "source": [
    "# print the feature names\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print (feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0]]\n"
     ]
    }
   ],
   "source": [
    "# extract features from new document using built vectorizer\n",
    "new_doc = ['I want to buy a car']\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print(new_doc_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a data frame from the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   because  because it  blue  blue car  blue eyes   buy  buy green   car  \\\n",
      "0     0.00        0.00  0.00      0.00       0.00  0.00       0.00  0.23   \n",
      "1     0.00        0.00  0.27      0.34       0.00  0.00       0.00  0.22   \n",
      "2     0.00        0.00  0.00      0.00       0.00  0.00       0.00  0.24   \n",
      "3     0.25        0.25  0.20      0.00       0.25  0.25       0.25  0.00   \n",
      "\n",
      "   car has  car is  ...  mileage    my  my blue  shirt  shirt because   the  \\\n",
      "0     0.00    0.36  ...     0.00  0.00     0.00   0.00           0.00  0.29   \n",
      "1     0.27    0.00  ...     0.27  0.00     0.00   0.00           0.00  0.27   \n",
      "2     0.29    0.00  ...     0.29  0.00     0.00   0.00           0.00  0.00   \n",
      "3     0.00    0.00  ...     0.00  0.25     0.25   0.25           0.25  0.00   \n",
      "\n",
      "   the blue  the green  your  your car  \n",
      "0      0.00       0.36  0.00      0.00  \n",
      "1      0.34       0.00  0.00      0.00  \n",
      "2      0.00       0.00  0.37      0.37  \n",
      "3      0.00       0.00  0.00      0.00  \n",
      "\n",
      "[4 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(data=features, columns=feature_names)\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   because  because it  blue  blue car  blue eyes   buy  buy green   car  \\\n",
      "0     0.00        0.00  0.00      0.00       0.00  0.00       0.00  0.23   \n",
      "1     0.00        0.00  0.27      0.34       0.00  0.00       0.00  0.22   \n",
      "2     0.00        0.00  0.00      0.00       0.00  0.00       0.00  0.24   \n",
      "3     0.25        0.25  0.20      0.00       0.25  0.25       0.25  0.00   \n",
      "\n",
      "   car has  car is  ...  mileage    my  my blue  shirt  shirt because   the  \\\n",
      "0     0.00    0.36  ...     0.00  0.00     0.00   0.00           0.00  0.29   \n",
      "1     0.27    0.00  ...     0.27  0.00     0.00   0.00           0.00  0.27   \n",
      "2     0.29    0.00  ...     0.29  0.00     0.00   0.00           0.00  0.00   \n",
      "3     0.00    0.00  ...     0.00  0.25     0.25   0.25           0.25  0.00   \n",
      "\n",
      "   the blue  the green  your  your car  \n",
      "0      0.00       0.36  0.00      0.00  \n",
      "1      0.34       0.00  0.00      0.00  \n",
      "2      0.00       0.00  0.37      0.37  \n",
      "3      0.00       0.00  0.00      0.00  \n",
      "\n",
      "[4 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(norm='l2',smooth_idf=True, use_idf=True) #smoothing gives sme weights to terms with zero idf not to ignore them.\n",
    "tfidf_matrix = transformer.fit_transform(bow_features)\n",
    "features = np.round(tfidf_matrix.todense(), 2)\n",
    " \n",
    "df_tfidf = pd.DataFrame(data=features, columns=feature_names)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   because  because it  blue  blue car  blue eyes   buy  buy green   car  \\\n",
      "0      0.0         0.0   0.0       0.0        0.0  0.71        0.0  0.71   \n",
      "\n",
      "   car has  car is  ...  mileage   my  my blue  shirt  shirt because  the  \\\n",
      "0      0.0     0.0  ...      0.0  0.0      0.0    0.0            0.0  0.0   \n",
      "\n",
      "   the blue  the green  your  your car  \n",
      "0       0.0        0.0   0.0       0.0  \n",
      "\n",
      "[1 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# tfidf features for new_doc\n",
    "tfidf_matrix = transformer.fit_transform(new_doc_features)\n",
    "features_new = np.round(tfidf_matrix.todense(), 2)\n",
    " \n",
    "df_tfidf_new = pd.DataFrame(data=features_new, columns=feature_names)\n",
    "print(df_tfidf_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = ['the green car is fast',\n",
    " 'The blue car has low mileage',\n",
    " 'Your car has high mileage',\n",
    " 'I buy green shirt because it fits my blue eyes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Your', 'car', 'has', 'high', 'mileage']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# tokenize corpora\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in CORPUS]\n",
    "new_doc = ['Your car has high mileage']\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n",
    "print(tokenized_new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08123658  0.08754414 -0.0110436   0.07498942 -0.03862057]\n",
      "[-0.00609899 -0.0886431  -0.02505467 -0.07300099 -0.05474317]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# build the word2vec model on our training corpus\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, size=5, window=2, min_count=1)\n",
    "print (model.wv['green'])\n",
    "print (model.wv['fast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "common_texts[0:3]\n",
    "[['human', 'interface', 'computer'],\n",
    " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    " ['eps', 'user', 'interface', 'system']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_texts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(common_texts)# we assigned a unique integer id to all words appearing in the corpus with the gensim Dictionary class.\n",
    "print(dictionary.token2id) # the mapping between words and their ids\n",
    "corpus = [dictionary.doc2bow(text) for text in common_texts] # each word has an Id and each document has words\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['human', 'interface', 'computer'], tags=[0]), TaggedDocument(words=['survey', 'user', 'computer', 'system', 'response', 'time'], tags=[1]), TaggedDocument(words=['eps', 'user', 'interface', 'system'], tags=[2]), TaggedDocument(words=['system', 'human', 'system', 'eps'], tags=[3]), TaggedDocument(words=['user', 'response', 'time'], tags=[4]), TaggedDocument(words=['trees'], tags=[5]), TaggedDocument(words=['graph', 'trees'], tags=[6]), TaggedDocument(words=['graph', 'minors', 'trees'], tags=[7]), TaggedDocument(words=['graph', 'minors', 'survey'], tags=[8])]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0014358  -0.0090469  -0.02318968  0.06386533  0.03190885]\n"
     ]
    }
   ],
   "source": [
    "#  building a model\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, dm =1)\n",
    "#Infer vector for a new document:\n",
    "vector = model.infer_vector([\"system\", \"response\"])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.7513596415519714), (2, 0.6718040704727173), (6, 0.5921541452407837), (8, 0.1333676278591156), (4, -0.15636000037193298), (7, -0.3663793206214905), (1, -0.505468487739563), (3, -0.5580600500106812)]\n",
      "[-0.08706785 -0.08330474  0.06607319  0.09561814 -0.00277567]\n"
     ]
    }
   ],
   "source": [
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar(0)\n",
    "print(similar_doc) # the most similar documents to the first document\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
